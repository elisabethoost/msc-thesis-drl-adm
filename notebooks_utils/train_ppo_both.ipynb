{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 10 scenarios\n",
      "Using device: mps\n",
      "Training on 3ac-10-mixed\n",
      "Getting model version for 3ac-10-mixed\n",
      "Getting model version for 3ac-10-mixed\n",
      "Models will be saved to:\n",
      "   ../trained_models/ppo/myopic_3ac-10-mixed-10-1.zip\n",
      "   ../trained_models/ppo/proactive_3ac-10-mixed-10-1.zip\n",
      "Results directory created at: ../results/ppo/20241208-11-25\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import platform\n",
    "import re\n",
    "import subprocess\n",
    "import torch as th\n",
    "import pickle\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from scripts.utils import (\n",
    "    load_scenario_data,\n",
    "    verify_training_folders,\n",
    "    create_results_directory,\n",
    "    get_model_version,\n",
    "    format_days,\n",
    "    calculate_training_days,\n",
    "    initialize_device,\n",
    "    check_device_capabilities,\n",
    "    get_device_info,\n",
    ")\n",
    "from scripts.visualizations import *\n",
    "from src.config import *\n",
    "from sb3_contrib import MaskablePPO\n",
    "from stable_baselines3.common.logger import configure\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from src.environment import AircraftDisruptionEnv\n",
    "from sb3_contrib.common.maskable.utils import get_action_masks\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\", font_scale=1.5)\n",
    "\n",
    "\n",
    "if 'MAX_TOTAL_TIMESTEPS' not in globals():\n",
    "    MAX_TOTAL_TIMESTEPS = 100000\n",
    "if 'TRAINING_FOLDERS_PATH' not in globals():\n",
    "    TRAINING_FOLDERS_PATH = \"../data/Training/3ac-1000-mixed/\"\n",
    "if 'SEEDS' not in globals():\n",
    "    SEEDS = [0]\n",
    "if 'brute_force_flag' not in globals():\n",
    "    brute_force_flag = False\n",
    "if 'cross_val_flag' not in globals():\n",
    "    cross_val_flag = False\n",
    "if 'early_stopping_flag' not in globals():\n",
    "    early_stopping_flag = False\n",
    "if 'TESTING_FOLDERS_PATH' not in globals():\n",
    "    TESTING_FOLDERS_PATH = \"../data/Testing/3ac-1-mixed/\"\n",
    "if 'CROSS_VAL_INTERVAL' not in globals():\n",
    "    CROSS_VAL_INTERVAL = 100000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "LEARNING_RATE = 2.5e-4\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 64\n",
    "NEURAL_NET_STRUCTURE = dict(net_arch=[256, 256])\n",
    "LEARNING_STARTS = 0\n",
    "TRAIN_FREQ = 4\n",
    "N_STEPS = 2048\n",
    "\n",
    "\n",
    "\n",
    "# calculate how many folders are in the training folder\n",
    "num_folders = len([f for f in os.listdir(TRAINING_FOLDERS_PATH) if os.path.isdir(os.path.join(TRAINING_FOLDERS_PATH, f))])\n",
    "print(f\"Training on {num_folders} scenarios\")\n",
    "\n",
    "device = initialize_device()\n",
    "stripped_scenario_folder = TRAINING_FOLDERS_PATH.split(\"/\")[-2]\n",
    "print(f\"Training on {stripped_scenario_folder}\")\n",
    "\n",
    "num_days_trained_on = 10\n",
    "formatted_days = format_days(num_days_trained_on)\n",
    "model_name = os.path.basename(os.path.normpath(TRAINING_FOLDERS_PATH))\n",
    "MODEL_SAVE_PATH = '../trained_models/ppo/'\n",
    "MODEL_SAVE_NAME_MYOPIC = f'{model_name}-{formatted_days}-{get_model_version(model_name, \"myopic\", \"ppo\")}.zip'\n",
    "MODEL_SAVE_NAME_PROACTIVE = f'{model_name}-{formatted_days}-{get_model_version(model_name, \"proactive\", \"ppo\")}.zip'\n",
    "print(f\"Models will be saved to:\\n   {MODEL_SAVE_PATH}myopic_{MODEL_SAVE_NAME_MYOPIC}\\n   {MODEL_SAVE_PATH}proactive_{MODEL_SAVE_NAME_PROACTIVE}\")\n",
    "\n",
    "results_dir = create_results_directory(append_to_name='ppo')\n",
    "print(f\"Results directory created at: {results_dir}\")\n",
    "\n",
    "# Store episode-level rewards\n",
    "episode_rewards_myopic = []\n",
    "episode_rewards_proactive = []\n",
    "episode_steps_myopic = []\n",
    "episode_steps_proactive = []\n",
    "\n",
    "def my_get_action_masks(env):\n",
    "    mask = env.get_action_mask()\n",
    "    mask = np.array(mask, dtype=np.uint8)\n",
    "    return mask\n",
    "\n",
    "def train_ppo_agent(env_type):\n",
    "    scenario_folders = [\n",
    "        os.path.join(TRAINING_FOLDERS_PATH, folder)\n",
    "        for folder in os.listdir(TRAINING_FOLDERS_PATH)\n",
    "        if os.path.isdir(os.path.join(TRAINING_FOLDERS_PATH, folder))\n",
    "    ]\n",
    "    \n",
    "    class ScenarioEnvWrapper(gym.Env):\n",
    "        def __init__(self, scenario_folders, env_type):\n",
    "            super(ScenarioEnvWrapper, self).__init__()\n",
    "            self.scenario_folders = scenario_folders\n",
    "            self.env_type = env_type\n",
    "            self.current_scenario_idx = -1\n",
    "\n",
    "            self.episode_scenario_rewards = []\n",
    "            self.episode_count = 0\n",
    "            \n",
    "            # Track total environment steps\n",
    "            self.total_env_steps = 0\n",
    "\n",
    "            self.load_next_scenario()\n",
    "            self.observation_space = self.env.observation_space\n",
    "            self.action_space = self.env.action_space\n",
    "            self.episode_reward = 0\n",
    "\n",
    "        def load_next_scenario(self):\n",
    "            self.current_scenario_idx = (self.current_scenario_idx + 1) % len(self.scenario_folders)\n",
    "            scenario_folder = self.scenario_folders[self.current_scenario_idx]\n",
    "            data_dict = load_scenario_data(scenario_folder)\n",
    "            aircraft_dict = data_dict['aircraft']\n",
    "            flights_dict = data_dict['flights']\n",
    "            rotations_dict = data_dict['rotations']\n",
    "            alt_aircraft_dict = data_dict['alt_aircraft']\n",
    "            config_dict = data_dict['config']\n",
    "\n",
    "            self.env = AircraftDisruptionEnv(\n",
    "                aircraft_dict,\n",
    "                flights_dict,\n",
    "                rotations_dict,\n",
    "                alt_aircraft_dict,\n",
    "                config_dict,\n",
    "                env_type=self.env_type\n",
    "            )\n",
    "\n",
    "        def reset(self, seed=None, options=None):\n",
    "            # If not the first reset, we finished one scenario\n",
    "            if self.episode_reward != 0:\n",
    "                self.episode_scenario_rewards.append(self.episode_reward)\n",
    "                self.episode_reward = 0\n",
    "\n",
    "            # If we completed a full cycle of scenarios, one meta-episode is done\n",
    "            if len(self.episode_scenario_rewards) == len(self.scenario_folders):\n",
    "                total_episode_reward = sum(self.episode_scenario_rewards)\n",
    "                # Record total steps and rewards depending on env_type\n",
    "                if self.env_type == \"myopic\":\n",
    "                    episode_rewards_myopic.append(total_episode_reward)\n",
    "                    episode_steps_myopic.append(self.total_env_steps)\n",
    "                else:\n",
    "                    episode_rewards_proactive.append(total_episode_reward)\n",
    "                    episode_steps_proactive.append(self.total_env_steps)\n",
    "\n",
    "                # Reset for next meta-episode\n",
    "                self.episode_scenario_rewards = []\n",
    "                self.episode_count += 1\n",
    "\n",
    "            self.load_next_scenario()\n",
    "            obs, info = self.env.reset()\n",
    "            return obs, info\n",
    "\n",
    "        def step(self, action):\n",
    "            obs, reward, done, truncated, info = self.env.step(action)\n",
    "            self.episode_reward += reward\n",
    "            self.total_env_steps += 1  # increment step counter each step\n",
    "            return obs, reward, done, truncated, info\n",
    "\n",
    "        def render(self, mode='human'):\n",
    "            return self.env.render(mode=mode)\n",
    "\n",
    "        def close(self):\n",
    "            return self.env.close()\n",
    "\n",
    "        def get_action_mask(self):\n",
    "            return self.env.get_action_mask()\n",
    "\n",
    "    env = ScenarioEnvWrapper(scenario_folders, env_type)\n",
    "    env = ActionMasker(env, my_get_action_masks)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "\n",
    "    model = MaskablePPO(\n",
    "        'MultiInputPolicy',\n",
    "        env,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        gamma=GAMMA,\n",
    "        verbose=1,\n",
    "        tensorboard_log=f\"./ppo_aircraft_tensorboard_{env_type}/\",\n",
    "        device=device,\n",
    "        policy_kwargs=NEURAL_NET_STRUCTURE,\n",
    "        n_steps=N_STEPS,\n",
    "    )\n",
    "\n",
    "    logger = configure()\n",
    "    model.set_logger(logger)\n",
    "\n",
    "    def log_rewards(_locals, _globals):\n",
    "        return True\n",
    "\n",
    "    # Run the training loop for a fixed number of timesteps\n",
    "    model.learn(total_timesteps=MAX_TOTAL_TIMESTEPS, use_masking=True, callback=log_rewards)\n",
    "\n",
    "    if env_type == \"myopic\":\n",
    "        model.save(f\"{MODEL_SAVE_PATH}myopic_{MODEL_SAVE_NAME_MYOPIC}\")\n",
    "    else:\n",
    "        model.save(f\"{MODEL_SAVE_PATH}proactive_{MODEL_SAVE_NAME_PROACTIVE}\")\n",
    "\n",
    "    return [], 0  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running training for seed 0...\n",
      "Using mps device\n",
      "Logging to /var/folders/m6/gwyqzldd12bg_s3mrl40tp6r0000gn/T/SB3-2024-12-08-11-25-02-130409\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 150  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 13   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 135          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003209161 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.86        |\n",
      "|    explained_variance   | 1.25e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.44e+07     |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00336     |\n",
      "|    value_loss           | 5.56e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 128          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 47           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005070317 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 5.61e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.31e+07     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00455     |\n",
      "|    value_loss           | 6.28e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 125          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 65           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002760808 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 7.53e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.17e+07     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00312     |\n",
      "|    value_loss           | 5.92e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 125          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 81           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002897782 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.83        |\n",
      "|    explained_variance   | 0.000156     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.23e+07     |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00334     |\n",
      "|    value_loss           | 6.5e+07      |\n",
      "------------------------------------------\n",
      "Using mps device\n",
      "Logging to /var/folders/m6/gwyqzldd12bg_s3mrl40tp6r0000gn/T/SB3-2024-12-08-11-26-27-408298\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 168  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 12   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 146           |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 27            |\n",
      "|    total_timesteps      | 4096          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00027572166 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.84         |\n",
      "|    explained_variance   | 1.25e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.66e+07      |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -0.00296      |\n",
      "|    value_loss           | 6.26e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 141           |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 43            |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00055252446 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.82         |\n",
      "|    explained_variance   | 5.17e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.86e+07      |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.00456      |\n",
      "|    value_loss           | 5.53e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 139          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 58           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003405964 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.85        |\n",
      "|    explained_variance   | 0.000143     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.18e+07     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00352     |\n",
      "|    value_loss           | 5.96e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 138          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 73           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006248035 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.85        |\n",
      "|    explained_variance   | 0.000204     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.63e+07     |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00531     |\n",
      "|    value_loss           | 6.09e+07     |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# Set the number of seeds\n",
    "\n",
    "NUM_SEEDS = len(SEEDS)\n",
    "# Prepare storage for multiple runs\n",
    "all_myopic_runs = []\n",
    "all_proactive_runs = []\n",
    "all_myopic_steps_runs = []\n",
    "all_proactive_steps_runs = []\n",
    "\n",
    "def run_training_for_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    global episode_rewards_myopic, episode_rewards_proactive\n",
    "    global episode_steps_myopic, episode_steps_proactive\n",
    "\n",
    "    episode_rewards_myopic = []\n",
    "    episode_rewards_proactive = []\n",
    "    episode_steps_myopic = []\n",
    "    episode_steps_proactive = []\n",
    "\n",
    "    # Run the training\n",
    "    train_ppo_agent('myopic')\n",
    "    train_ppo_agent('proactive')\n",
    "\n",
    "    return episode_rewards_myopic, episode_rewards_proactive, episode_steps_myopic, episode_steps_proactive\n",
    "\n",
    "# Run training for each seed\n",
    "for s in SEEDS:\n",
    "    print(f\"Running training for seed {s}...\")\n",
    "    myopic_rewards_seed, proactive_rewards_seed, myopic_steps_seed, proactive_steps_seed = run_training_for_seed(s)\n",
    "    all_myopic_runs.append(myopic_rewards_seed)\n",
    "    all_proactive_runs.append(proactive_rewards_seed)\n",
    "    all_myopic_steps_runs.append(myopic_steps_seed)\n",
    "    all_proactive_steps_runs.append(proactive_steps_seed)\n",
    "\n",
    "\n",
    "min_length_myopic = min(len(run) for run in all_myopic_runs)\n",
    "min_length_proactive = min(len(run) for run in all_proactive_runs)\n",
    "\n",
    "all_myopic_runs = [run[:min_length_myopic] for run in all_myopic_runs]\n",
    "all_proactive_runs = [run[:min_length_proactive] for run in all_proactive_runs]\n",
    "\n",
    "all_myopic_steps_runs = [run[:min_length_myopic] for run in all_myopic_steps_runs]\n",
    "all_proactive_steps_runs = [run[:min_length_proactive] for run in all_proactive_steps_runs]\n",
    "\n",
    "all_myopic_runs = np.array(all_myopic_runs) / num_folders   \n",
    "all_proactive_runs = np.array(all_proactive_runs) / num_folders\n",
    "\n",
    "all_myopic_steps_runs = np.array(all_myopic_steps_runs)\n",
    "all_proactive_steps_runs = np.array(all_proactive_steps_runs)\n",
    "\n",
    "myopic_mean = all_myopic_runs.mean(axis=0)\n",
    "myopic_std = all_myopic_runs.std(axis=0)\n",
    "\n",
    "proactive_mean = all_proactive_runs.mean(axis=0)\n",
    "proactive_std = all_proactive_runs.std(axis=0)\n",
    "\n",
    "myopic_steps_mean = all_myopic_steps_runs.mean(axis=0).astype(int)\n",
    "proactive_steps_mean = all_proactive_steps_runs.mean(axis=0).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_results\u001b[39m\u001b[38;5;124m\"\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Save the raw data\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_results/myopic_mean.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m, myopic_mean)\n\u001b[1;32m      9\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_results/myopic_std.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m, myopic_std)\n\u001b[1;32m     10\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_results/proactive_mean.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m, proactive_mean)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Create final_results directory if it doesn't exist\n",
    "os.makedirs(\"final_results\", exist_ok=True)\n",
    "\n",
    "# Save the raw data\n",
    "np.save(\"final_results/myopic_mean.npy\", myopic_mean)\n",
    "np.save(\"final_results/myopic_std.npy\", myopic_std)\n",
    "np.save(\"final_results/proactive_mean.npy\", proactive_mean)\n",
    "np.save(\"final_results/proactive_std.npy\", proactive_std)\n",
    "\n",
    "# Plotting reward vs steps\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "# Set smoothing window\n",
    "smooth_window = 25\n",
    "\n",
    "def smooth(data, window):\n",
    "    if window > 1 and len(data) >= window:\n",
    "        return np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "    return data\n",
    "\n",
    "myopic_mean_sm = smooth(myopic_mean, smooth_window)\n",
    "myopic_std_sm = smooth(myopic_std, smooth_window)\n",
    "myopic_steps_sm = myopic_steps_mean[:len(myopic_mean_sm)]\n",
    "\n",
    "proactive_mean_sm = smooth(proactive_mean, smooth_window)\n",
    "proactive_std_sm = smooth(proactive_std, smooth_window)\n",
    "proactive_steps_sm = proactive_steps_mean[:len(proactive_mean_sm)]\n",
    "\n",
    "plt.plot(myopic_steps_sm, myopic_mean_sm, label=\"Myopic PPO\", color='blue')\n",
    "plt.fill_between(myopic_steps_sm, myopic_mean_sm - myopic_std_sm, myopic_mean_sm + myopic_std_sm, alpha=0.2, color='blue')\n",
    "\n",
    "plt.plot(proactive_steps_sm, proactive_mean_sm, label=\"Proactive PPO\", color='orange')\n",
    "plt.fill_between(proactive_steps_sm, proactive_mean_sm - proactive_std_sm, proactive_mean_sm + proactive_std_sm, alpha=0.2, color='orange')\n",
    "\n",
    "plt.xlabel(\"Environment Steps (Frames)\")\n",
    "plt.ylabel(\"Episode Reward\")\n",
    "plt.title(f\"Averaged Episode Rewards over {len(SEEDS)} Seeds (PPO, {stripped_scenario_folder})\")\n",
    "plt.legend(frameon=False)\n",
    "plt.grid(True)\n",
    "\n",
    "plot_file = os.path.join(results_dir, \"averaged_rewards_over_steps.png\")\n",
    "plt.savefig(plot_file)\n",
    "plt.show()\n",
    "print(f\"Averaged reward vs steps plot saved to {plot_file}\")\n",
    "print(f\"Raw data saved to final_results/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
