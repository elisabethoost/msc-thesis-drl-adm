{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average reward over all scenarios for each model:\n",
      "myopic-training_63.zip      -4922.8722\n",
      "proactive-training_64.zip   -3577.9805\n",
      "reactive-training_65.zip    -7566.5315\n",
      "dtype: float64\n",
      "\n",
      "Standard Deviation of rewards for each model:\n",
      "myopic-training_63.zip       7963.590950\n",
      "proactive-training_64.zip    7955.569338\n",
      "reactive-training_65.zip     4616.836861\n",
      "dtype: float64\n",
      "\n",
      "Average runtime over all scenarios for each model:\n",
      "myopic-training_63.zip       0.004711\n",
      "proactive-training_64.zip    0.004488\n",
      "reactive-training_65.zip     0.004045\n",
      "dtype: float64\n",
      "\n",
      "Standard Deviation of runtime for each model:\n",
      "myopic-training_63.zip       0.001188\n",
      "proactive-training_64.zip    0.001108\n",
      "reactive-training_65.zip     0.000922\n",
      "dtype: float64\n",
      "\n",
      "Pairwise p-values for REWARDS (based on paired t-tests):\n",
      "                          myopic-training_63.zip proactive-training_64.zip  \\\n",
      "myopic-training_63.zip                       NaN                  0.000092   \n",
      "proactive-training_64.zip               0.000092                       NaN   \n",
      "reactive-training_65.zip                     0.0                       0.0   \n",
      "\n",
      "                          reactive-training_65.zip  \n",
      "myopic-training_63.zip                         0.0  \n",
      "proactive-training_64.zip                      0.0  \n",
      "reactive-training_65.zip                       NaN  \n",
      "\n",
      "Pairwise p-values for RUNTIME (based on paired t-tests):\n",
      "                          myopic-training_63.zip proactive-training_64.zip  \\\n",
      "myopic-training_63.zip                       NaN                       0.0   \n",
      "proactive-training_64.zip                    0.0                       NaN   \n",
      "reactive-training_65.zip                     0.0                       0.0   \n",
      "\n",
      "                          reactive-training_65.zip  \n",
      "myopic-training_63.zip                         0.0  \n",
      "proactive-training_64.zip                      0.0  \n",
      "reactive-training_65.zip                       NaN  \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stable_baselines3 import DQN\n",
    "import torch\n",
    "import time\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "from src.environment import AircraftDisruptionEnv\n",
    "from scripts.visualizations import StatePlotter\n",
    "from scripts.utils import load_scenario_data\n",
    "from src.config import *\n",
    "\n",
    "def run_inference_dqn(model_path, scenario_folder, env_type, seed):\n",
    "    # Load scenario data\n",
    "    data_dict = load_scenario_data(scenario_folder)\n",
    "    aircraft_dict = data_dict['aircraft']\n",
    "    flights_dict = data_dict['flights']\n",
    "    rotations_dict = data_dict['rotations']\n",
    "    alt_aircraft_dict = data_dict['alt_aircraft']\n",
    "    config_dict = data_dict['config']\n",
    "\n",
    "    # Initialize environment\n",
    "    env = AircraftDisruptionEnv(\n",
    "        aircraft_dict, \n",
    "        flights_dict, \n",
    "        rotations_dict, \n",
    "        alt_aircraft_dict, \n",
    "        config_dict,\n",
    "        env_type=env_type\n",
    "    )\n",
    "\n",
    "    # Load model\n",
    "    model = DQN.load(model_path)\n",
    "    model.set_env(env)\n",
    "    model.policy.set_training_mode(False)\n",
    "    model.exploration_rate = 0.0\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Reset the environment\n",
    "    obs, _ = env.reset()\n",
    "    done_flag = False\n",
    "    total_reward = 0\n",
    "    step_num = 0\n",
    "    max_steps = 1000\n",
    "\n",
    "    # Start timing the inference\n",
    "    start_time = time.time()\n",
    "\n",
    "    while not done_flag and step_num < max_steps:\n",
    "        # Convert observation to float32\n",
    "        obs = {key: np.array(value, dtype=np.float32) for key, value in obs.items()}\n",
    "\n",
    "        # Get action mask\n",
    "        action_mask = obs.get('action_mask', None)\n",
    "        if action_mask is None:\n",
    "            raise ValueError(\"Action mask is missing in the observation!\")\n",
    "\n",
    "        # Get Q-values\n",
    "        obs_tensor = model.policy.obs_to_tensor(obs)[0]\n",
    "        q_values = model.policy.q_net(obs_tensor).detach().cpu().numpy().squeeze()\n",
    "\n",
    "        # Mask invalid actions\n",
    "        masked_q_values = q_values.copy()\n",
    "        masked_q_values[action_mask == 0] = -np.inf\n",
    "\n",
    "        action = np.argmax(masked_q_values)\n",
    "\n",
    "        # Step environment\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        done_flag = terminated or truncated\n",
    "        step_num += 1\n",
    "\n",
    "    # End timing\n",
    "    end_time = time.time()\n",
    "    runtime = end_time - start_time\n",
    "\n",
    "    return total_reward, step_num, runtime\n",
    "\n",
    "model_to_compare = [\n",
    "    \"../trained_models/dqn/6ac-100-stochastic-high/42/training_63/myopic-training_63.zip\",\n",
    "    \"../trained_models/dqn/6ac-100-stochastic-high/42/training_64/proactive-training_64.zip\",\n",
    "    \"../trained_models/dqn/6ac-100-stochastic-high/42/training_65/reactive-training_65.zip\",\n",
    "]\n",
    "\n",
    "# Define the scenario folder set\n",
    "SCENARIO_FOLDER = \"../data/Testing/6ac-1000-stochastic-high/\"\n",
    "\n",
    "# Extract all scenario folders\n",
    "scenario_folders = [os.path.join(SCENARIO_FOLDER, d) for d in os.listdir(SCENARIO_FOLDER) \n",
    "                    if os.path.isdir(os.path.join(SCENARIO_FOLDER, d)) and d.lower().startswith(\"scenario_\")]\n",
    "scenario_folders.sort()\n",
    "\n",
    "def extract_env_type(model_path):\n",
    "    match = re.search(r'/(myopic|proactive|reactive)_', model_path)\n",
    "    return match.group(1) if match else \"proactive\"  # default to proactive if not found\n",
    "\n",
    "# Create DataFrames to store results\n",
    "results_df = pd.DataFrame(index=[os.path.basename(s) for s in scenario_folders],\n",
    "                          columns=[os.path.basename(m) for m in model_to_compare])\n",
    "runtime_df = pd.DataFrame(index=[os.path.basename(s) for s in scenario_folders],\n",
    "                          columns=[os.path.basename(m) for m in model_to_compare])\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "seed = int(time.time())\n",
    "\n",
    "# Run inference for each scenario and each model\n",
    "for scenario_folder in scenario_folders:\n",
    "    for model_path in model_to_compare:\n",
    "        if not os.path.exists(model_path):\n",
    "            raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
    "\n",
    "        if not os.path.exists(scenario_folder):\n",
    "            raise FileNotFoundError(f\"Scenario folder not found: {scenario_folder}\")\n",
    "\n",
    "        env_type = extract_env_type(model_path)\n",
    "        total_reward, steps, runtime = run_inference_dqn(model_path, scenario_folder, env_type, seed)\n",
    "        \n",
    "        scenario_name = os.path.basename(scenario_folder)\n",
    "        model_name = os.path.basename(model_path)\n",
    "        \n",
    "        # Store the result in DataFrames\n",
    "        results_df.loc[scenario_name, model_name] = total_reward\n",
    "        runtime_df.loc[scenario_name, model_name] = runtime\n",
    "\n",
    "# Convert to numeric\n",
    "results_df = results_df.apply(pd.to_numeric, errors='coerce')\n",
    "runtime_df = runtime_df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Compute statistics\n",
    "average_rewards = results_df.mean()\n",
    "std_rewards = results_df.std()\n",
    "\n",
    "average_runtime = runtime_df.mean()\n",
    "std_runtime = runtime_df.std()\n",
    "\n",
    "print(\"\\nAverage reward over all scenarios for each model:\")\n",
    "print(average_rewards)\n",
    "\n",
    "print(\"\\nStandard Deviation of rewards for each model:\")\n",
    "print(std_rewards)\n",
    "\n",
    "print(\"\\nAverage runtime over all scenarios for each model:\")\n",
    "print(average_runtime)\n",
    "\n",
    "print(\"\\nStandard Deviation of runtime for each model:\")\n",
    "print(std_runtime)\n",
    "\n",
    "# Perform pairwise t-tests for rewards and runtime\n",
    "# We'll test model_i vs model_j for each pair i < j\n",
    "model_names = results_df.columns\n",
    "n_models = len(model_names)\n",
    "\n",
    "print(\"\\nPairwise p-values for REWARDS (based on paired t-tests):\")\n",
    "p_value_rewards = pd.DataFrame(index=model_names, columns=model_names)\n",
    "for i in range(n_models):\n",
    "    for j in range(n_models):\n",
    "        if i == j:\n",
    "            p_value_rewards.iloc[i, j] = np.nan\n",
    "        else:\n",
    "            # Paired t-test since it's the same set of scenarios\n",
    "            stat, p_val = ttest_rel(results_df[model_names[i]], results_df[model_names[j]])\n",
    "            p_value_rewards.iloc[i, j] = p_val\n",
    "print(p_value_rewards)\n",
    "\n",
    "print(\"\\nPairwise p-values for RUNTIME (based on paired t-tests):\")\n",
    "p_value_runtime = pd.DataFrame(index=model_names, columns=model_names)\n",
    "for i in range(n_models):\n",
    "    for j in range(n_models):\n",
    "        if i == j:\n",
    "            p_value_runtime.iloc[i, j] = np.nan\n",
    "        else:\n",
    "            # Paired t-test\n",
    "            stat, p_val = ttest_rel(runtime_df[model_names[i]], runtime_df[model_names[j]])\n",
    "            p_value_runtime.iloc[i, j] = p_val\n",
    "print(p_value_runtime)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
